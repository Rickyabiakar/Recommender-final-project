from llama_cpp import Llama

# Initialize the LLM model
llm = Llama(
    model_path="models/llm/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf",  # Ensure file path and name are correct
    n_ctx=1024,
    verbose=True  # Shows logs during execution
)

def explain_recommendations(book_list):
    """
    Generate a natural language explanation for a list of recommended books using an LLM.

    Args:
        book_list (list of str): List of book titles recommended to the user.

    Returns:
        str: A short explanation generated by the language model, or an error message if generation fails.
    """
    prompt = (
        "A user liked these books:\n"
        + "\n".join(f"- {title}" for title in book_list)
        + "\n\nWhy do these recommended books make sense based on the user's interests?\n"
        "Give a short explanation:"
    )
    try:
        result = llm(prompt, max_tokens=150)
        return result['choices'][0]['text'].strip()
    except Exception as e:
        return f"⚠️ LLM failed to generate a response: {e}"

