"""Module to generate LLM-based explanations for book recommendations."""

from llama_cpp import Llama

# Initialize the LLM model
llm = Llama(
    model_path=("models/llm/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"),
    n_ctx=1024,
    verbose=True,
)


def explain_recommendations(book_list):
    """
    Generate a natural language explanation for a list of recommended books.

    Args:
        book_list (list of str): Titles of recommended books.

    Returns:
        str: Explanation generated by the language model or error message.
    """
    prompt = (
        "A user liked these books:\n"
        + "\n".join(f"- {title}" for title in book_list)
        + "\n\nWhy do these recommended books make sense "
        "based on the user's interests?\nGive a short explanation:"
    )
    try:
        result = llm(prompt, max_tokens=150)
        return result["choices"][0]["text"].strip()
    except (KeyError, TypeError, RuntimeError) as error:
        return f"⚠️ LLM failed to generate a response: {error}"
